{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e0e716",
   "metadata": {},
   "source": [
    "## Building a Language Model from Scratch - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76724947",
   "metadata": {},
   "source": [
    "**Note:** The only difference from Part 1 to Part 1.5 is that we are using a dataloader and training in batches\n",
    "\n",
    "**Note:** The only difference from Part 1 to Part 2 is that we now including a context length of 5 and using a trick to keep the same rough network architecture working even though we are observing five tokens at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c445ef3",
   "metadata": {},
   "source": [
    "We are going to customize our tokenizer a little bit to only keep the most common words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bde40b-fe31-4c4c-a612-8e40959ca8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:45:41.688693Z",
     "iopub.status.busy": "2023-09-26T19:45:41.688266Z",
     "iopub.status.idle": "2023-09-26T19:45:41.694033Z",
     "shell.execute_reply": "2023-09-26T19:45:41.693321Z",
     "shell.execute_reply.started": "2023-09-26T19:45:41.688666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8b7e82-627a-4176-a835-473336d59b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:42:37.123053Z",
     "iopub.status.busy": "2023-09-26T19:42:37.122688Z",
     "iopub.status.idle": "2023-09-26T19:42:42.502094Z",
     "shell.execute_reply": "2023-09-26T19:42:42.500665Z",
     "shell.execute_reply.started": "2023-09-26T19:42:37.123024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77dd1edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:42:44.320332Z",
     "iopub.status.busy": "2023-09-26T19:42:44.319969Z",
     "iopub.status.idle": "2023-09-26T19:42:44.325699Z",
     "shell.execute_reply": "2023-09-26T19:42:44.324656Z",
     "shell.execute_reply.started": "2023-09-26T19:42:44.320304Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_common_words():\n",
    "    vocabulary = []\n",
    "    with open('common_words.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            if(not line.startswith(\"#!\")):\n",
    "                vocabulary.append(line.strip())\n",
    "    vocabulary.append(\"<pad>\")\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d764204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:42:44.592727Z",
     "iopub.status.busy": "2023-09-26T19:42:44.592069Z",
     "iopub.status.idle": "2023-09-26T19:42:44.597049Z",
     "shell.execute_reply": "2023-09-26T19:42:44.596221Z",
     "shell.execute_reply.started": "2023-09-26T19:42:44.592694Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    vocabulary = load_common_words()\n",
    "    tokens = word_tokenize(text)\n",
    "    return [token for token in tokens if token in vocabulary], vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b51d9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:42:45.313690Z",
     "iopub.status.busy": "2023-09-26T19:42:45.313348Z",
     "iopub.status.idle": "2023-09-26T19:42:45.360938Z",
     "shell.execute_reply": "2023-09-26T19:42:45.359908Z",
     "shell.execute_reply.started": "2023-09-26T19:42:45.313664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'word']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence for word tokenization.\"\n",
    "tokens, vocabulary = tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721bee6",
   "metadata": {},
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5066edd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:45:49.046901Z",
     "iopub.status.busy": "2023-09-26T19:45:49.046074Z",
     "iopub.status.idle": "2023-09-26T19:45:49.067209Z",
     "shell.execute_reply": "2023-09-26T19:45:49.066466Z",
     "shell.execute_reply.started": "2023-09-26T19:45:49.046869Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('shakespeare.txt', 'r') as file:\n",
    "        shakespeare = file.read()\n",
    "        return shakespeare\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222f63bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:45:51.285412Z",
     "iopub.status.busy": "2023-09-26T19:45:51.285075Z",
     "iopub.status.idle": "2023-09-26T19:46:57.002017Z",
     "shell.execute_reply": "2023-09-26T19:46:57.001300Z",
     "shell.execute_reply.started": "2023-09-26T19:45:51.285387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', 'Before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
     ]
    }
   ],
   "source": [
    "tokens, vocabulary = tokenize(dataset)\n",
    "\n",
    "print(tokens[0:10]) # print the first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb7725",
   "metadata": {},
   "source": [
    "**NEW:** We need a way now to grab up to five items from a given token index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7809e121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:46:59.652160Z",
     "iopub.status.busy": "2023-09-26T19:46:59.651819Z",
     "iopub.status.idle": "2023-09-26T19:46:59.658401Z",
     "shell.execute_reply": "2023-09-26T19:46:59.657552Z",
     "shell.execute_reply.started": "2023-09-26T19:46:59.652136Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokens_to_indexes(tokens, vocabulary):\n",
    "    return [vocabulary.index(token) for token in tokens]\n",
    "\n",
    "def fetch_context_window(token_indexes, i, window_size=3):\n",
    "    total_tokens = len(token_indexes)\n",
    "    \n",
    "    first_index = i+1-window_size\n",
    "    last_index = i+1\n",
    "\n",
    "    missing_tokens = -1 * first_index if first_index < 0 else 0\n",
    "    \n",
    "    \n",
    "    for j in range(missing_tokens):\n",
    "        first_index += 1\n",
    "        \n",
    "    context = token_indexes[first_index:last_index]\n",
    "\n",
    "    return context\n",
    "\n",
    "def pad_sequence(sequence, length, pad_with):\n",
    "    \n",
    "    size = len(sequence)\n",
    "    missing = length - len(sequence)\n",
    "    \n",
    "    new_sequence = []\n",
    "    \n",
    "    for i in range(missing):\n",
    "        new_sequence.append(pad_with)\n",
    "        \n",
    "    new_sequence = new_sequence + sequence\n",
    "    \n",
    "    return new_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827f720e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:47:02.227539Z",
     "iopub.status.busy": "2023-09-26T19:47:02.227202Z",
     "iopub.status.idle": "2023-09-26T19:47:02.266588Z",
     "shell.execute_reply": "2023-09-26T19:47:02.265636Z",
     "shell.execute_reply.started": "2023-09-26T19:47:02.227513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 135, 37]\n",
      "[135, 37, 72, 3776, 179]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I am so very hungry. What is there to eat?\"\n",
    "\n",
    "test_tokens, vocabulary = tokenize(test_text)\n",
    "test_token_indexes = tokens_to_indexes(test_tokens, vocabulary)\n",
    "window_1 = fetch_context_window(test_token_indexes, 2, window_size=5) # fetches up to the window size\n",
    "window_2 = fetch_context_window(test_token_indexes, 5, window_size=5) # fetches up to the window size\n",
    "\n",
    "print(window_1)\n",
    "print(window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f171b9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:47:04.470359Z",
     "iopub.status.busy": "2023-09-26T19:47:04.469527Z",
     "iopub.status.idle": "2023-09-26T19:47:04.478848Z",
     "shell.execute_reply": "2023-09-26T19:47:04.477811Z",
     "shell.execute_reply.started": "2023-09-26T19:47:04.470326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98913, 98913, 7, 135, 37]\n",
      "[135, 37, 72, 3776, 179]\n"
     ]
    }
   ],
   "source": [
    "padded_window_1 = pad_sequence(window_1, 5, vocabulary.index(\"<pad>\"))\n",
    "padded_window_2 = pad_sequence(window_2, 5, vocabulary.index(\"<pad>\"))\n",
    "\n",
    "print(padded_window_1)\n",
    "print(padded_window_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4a677",
   "metadata": {},
   "source": [
    "This section is new. This section will build a dataset using Pytorch and then use DataLoader to give us 32 examples at a time instead of just one example at a time. This will signficantly enhance performance when using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67b84f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T19:59:38.812657Z",
     "iopub.status.busy": "2023-09-26T19:59:38.812311Z",
     "iopub.status.idle": "2023-09-26T20:02:05.478891Z",
     "shell.execute_reply": "2023-09-26T20:02:05.478029Z",
     "shell.execute_reply.started": "2023-09-26T19:59:38.812632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset: 0.00% - 0/198756\n",
      "Creating dataset: 5.03% - 10000/198756\n",
      "Creating dataset: 10.06% - 20000/198756\n",
      "Creating dataset: 15.09% - 30000/198756\n",
      "Creating dataset: 20.13% - 40000/198756\n",
      "Creating dataset: 25.16% - 50000/198756\n",
      "Creating dataset: 30.19% - 60000/198756\n",
      "Creating dataset: 35.22% - 70000/198756\n",
      "Creating dataset: 40.25% - 80000/198756\n",
      "Creating dataset: 45.28% - 90000/198756\n",
      "Creating dataset: 50.31% - 100000/198756\n",
      "Creating dataset: 55.34% - 110000/198756\n",
      "Creating dataset: 60.38% - 120000/198756\n",
      "Creating dataset: 65.41% - 130000/198756\n",
      "Creating dataset: 70.44% - 140000/198756\n",
      "Creating dataset: 75.47% - 150000/198756\n",
      "Creating dataset: 80.50% - 160000/198756\n",
      "Creating dataset: 85.53% - 170000/198756\n",
      "Creating dataset: 90.56% - 180000/198756\n",
      "Creating dataset: 95.59% - 190000/198756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([21327,  1536,  1536,  ...,  7458,  7458,  7458], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "full_X = [] # will hold a list of token indexes for the training data\n",
    "full_y = [] # will hold a list of token indexes for the correct next word\n",
    "\n",
    "padding_token_index = vocabulary.index(\"<pad>\")\n",
    "token_indexes = tokens_to_indexes(tokens, vocabulary)\n",
    "\n",
    "for i in range(len(tokens)-1):\n",
    "    if i % 10000 == 0:\n",
    "        percentage = i*100.0 / len(tokens)\n",
    "        print(f\"Creating dataset: {percentage:.2f}% - {i}/{len(tokens)}\")\n",
    "    \n",
    "    windows = {}\n",
    "    \n",
    "    for j in range(1,window_size+1):\n",
    "        window = fetch_context_window(token_indexes, i, window_size=j)\n",
    "        windows[len(window)]=window\n",
    "                \n",
    "    for length, window in windows.items():\n",
    "        full_X.append(pad_sequence(window, window_size, padding_token_index))\n",
    "        full_y.append(vocabulary.index(tokens[i+1]))\n",
    "        \n",
    "full_X = torch.tensor(full_X)\n",
    "full_y = torch.tensor(full_y)\n",
    "\n",
    "full_X = full_X.to(device)\n",
    "full_y = full_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74e6df0c-4944-49e5-8a98-e79526e04df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T20:07:21.413383Z",
     "iopub.status.busy": "2023-09-26T20:07:21.412611Z",
     "iopub.status.idle": "2023-09-26T20:07:21.417470Z",
     "shell.execute_reply": "2023-09-26T20:07:21.416822Z",
     "shell.execute_reply.started": "2023-09-26T20:07:21.413353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset from your data\n",
    "dataset = TensorDataset(full_X, full_y)\n",
    "\n",
    "# Create a dataloader. This can handle batching\n",
    "dataload = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4affc12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T20:07:24.382751Z",
     "iopub.status.busy": "2023-09-26T20:07:24.382024Z",
     "iopub.status.idle": "2023-09-26T20:07:24.389124Z",
     "shell.execute_reply": "2023-09-26T20:07:24.387771Z",
     "shell.execute_reply.started": "2023-09-26T20:07:24.382718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c2604",
   "metadata": {},
   "source": [
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48bc3f",
   "metadata": {},
   "source": [
    "## Our Model in Brief:\n",
    "\n",
    "- Architecture: $X \\cdot E \\cdot O$\n",
    "    - where $X$ is a $(3 \\times \\textit{vocabulary_size})$ one-hot encoded vector for our input\n",
    "    - $E$ is a $(\\textit{vocabulary_size} \\times k)$ learnable matrix\n",
    "    - $O$ is a $(k \\times \\textit{vocabulary_size})$ learnable matrix\n",
    "- Loss: Cross Entropy Loss (common for language modeling and classification tasks)\n",
    "- Hyper-parameters: $k=100$ for embedding size, $lr=0.1$ for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d409aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T20:07:27.603405Z",
     "iopub.status.busy": "2023-09-26T20:07:27.602484Z",
     "iopub.status.idle": "2023-09-26T20:24:12.643191Z",
     "shell.execute_reply": "2023-09-26T20:24:12.641968Z",
     "shell.execute_reply.started": "2023-09-26T20:07:27.603363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/19876, Loss: 12.55, LR: 0.10\n",
      "Batch: 50/19876, Loss: 11.97, LR: 0.10\n",
      "Batch: 100/19876, Loss: 11.32, LR: 0.10\n",
      "Batch: 150/19876, Loss: 11.11, LR: 0.10\n",
      "Batch: 200/19876, Loss: 10.49, LR: 0.10\n",
      "Batch: 250/19876, Loss: 10.87, LR: 0.10\n",
      "Batch: 300/19876, Loss: 9.78, LR: 0.10\n",
      "Batch: 350/19876, Loss: 9.72, LR: 0.10\n",
      "Batch: 400/19876, Loss: 9.84, LR: 0.10\n",
      "Batch: 450/19876, Loss: 9.66, LR: 0.10\n",
      "Batch: 500/19876, Loss: 9.21, LR: 0.10\n",
      "Batch: 550/19876, Loss: 8.92, LR: 0.10\n",
      "Batch: 600/19876, Loss: 9.27, LR: 0.10\n",
      "Batch: 650/19876, Loss: 9.54, LR: 0.10\n",
      "Batch: 700/19876, Loss: 9.31, LR: 0.10\n",
      "Batch: 750/19876, Loss: 9.33, LR: 0.10\n",
      "Batch: 800/19876, Loss: 8.40, LR: 0.10\n",
      "Batch: 850/19876, Loss: 9.09, LR: 0.10\n",
      "Batch: 900/19876, Loss: 8.89, LR: 0.10\n",
      "Batch: 950/19876, Loss: 9.22, LR: 0.10\n",
      "Batch: 1000/19876, Loss: 9.46, LR: 0.10\n",
      "Batch: 1050/19876, Loss: 8.60, LR: 0.10\n",
      "Batch: 1100/19876, Loss: 8.44, LR: 0.10\n",
      "Batch: 1150/19876, Loss: 9.12, LR: 0.10\n",
      "Batch: 1200/19876, Loss: 9.31, LR: 0.10\n",
      "Batch: 1250/19876, Loss: 8.37, LR: 0.10\n",
      "Batch: 1300/19876, Loss: 8.53, LR: 0.10\n",
      "Batch: 1350/19876, Loss: 8.80, LR: 0.10\n",
      "Batch: 1400/19876, Loss: 8.21, LR: 0.10\n",
      "Batch: 1450/19876, Loss: 8.64, LR: 0.10\n",
      "Batch: 1500/19876, Loss: 7.86, LR: 0.10\n",
      "Batch: 1550/19876, Loss: 8.05, LR: 0.10\n",
      "Batch: 1600/19876, Loss: 8.98, LR: 0.10\n",
      "Batch: 1650/19876, Loss: 8.28, LR: 0.10\n",
      "Batch: 1700/19876, Loss: 8.16, LR: 0.10\n",
      "Batch: 1750/19876, Loss: 8.58, LR: 0.10\n",
      "Batch: 1800/19876, Loss: 8.17, LR: 0.10\n",
      "Batch: 1850/19876, Loss: 8.78, LR: 0.10\n",
      "Batch: 1900/19876, Loss: 8.04, LR: 0.10\n",
      "Batch: 1950/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 2000/19876, Loss: 8.57, LR: 0.10\n",
      "Batch: 2050/19876, Loss: 8.64, LR: 0.10\n",
      "Batch: 2100/19876, Loss: 7.86, LR: 0.10\n",
      "Batch: 2150/19876, Loss: 8.27, LR: 0.10\n",
      "Batch: 2200/19876, Loss: 7.75, LR: 0.10\n",
      "Batch: 2250/19876, Loss: 7.83, LR: 0.10\n",
      "Batch: 2300/19876, Loss: 7.99, LR: 0.10\n",
      "Batch: 2350/19876, Loss: 8.46, LR: 0.10\n",
      "Batch: 2400/19876, Loss: 7.89, LR: 0.10\n",
      "Batch: 2450/19876, Loss: 8.24, LR: 0.10\n",
      "Batch: 2500/19876, Loss: 8.33, LR: 0.10\n",
      "Batch: 2550/19876, Loss: 8.00, LR: 0.10\n",
      "Batch: 2600/19876, Loss: 8.16, LR: 0.10\n",
      "Batch: 2650/19876, Loss: 8.31, LR: 0.10\n",
      "Batch: 2700/19876, Loss: 7.77, LR: 0.10\n",
      "Batch: 2750/19876, Loss: 7.85, LR: 0.10\n",
      "Batch: 2800/19876, Loss: 8.44, LR: 0.10\n",
      "Batch: 2850/19876, Loss: 8.57, LR: 0.10\n",
      "Batch: 2900/19876, Loss: 6.99, LR: 0.10\n",
      "Batch: 2950/19876, Loss: 8.45, LR: 0.10\n",
      "Batch: 3000/19876, Loss: 7.63, LR: 0.10\n",
      "Batch: 3050/19876, Loss: 8.41, LR: 0.10\n",
      "Batch: 3100/19876, Loss: 7.72, LR: 0.10\n",
      "Batch: 3150/19876, Loss: 7.33, LR: 0.10\n",
      "Batch: 3200/19876, Loss: 8.14, LR: 0.10\n",
      "Batch: 3250/19876, Loss: 7.94, LR: 0.10\n",
      "Batch: 3300/19876, Loss: 8.39, LR: 0.10\n",
      "Batch: 3350/19876, Loss: 7.95, LR: 0.10\n",
      "Batch: 3400/19876, Loss: 8.23, LR: 0.10\n",
      "Batch: 3450/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 3500/19876, Loss: 6.86, LR: 0.10\n",
      "Batch: 3550/19876, Loss: 7.80, LR: 0.10\n",
      "Batch: 3600/19876, Loss: 7.96, LR: 0.10\n",
      "Batch: 3650/19876, Loss: 8.00, LR: 0.10\n",
      "Batch: 3700/19876, Loss: 8.30, LR: 0.10\n",
      "Batch: 3750/19876, Loss: 7.44, LR: 0.10\n",
      "Batch: 3800/19876, Loss: 8.32, LR: 0.10\n",
      "Batch: 3850/19876, Loss: 7.98, LR: 0.10\n",
      "Batch: 3900/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 3950/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 4000/19876, Loss: 7.71, LR: 0.10\n",
      "Batch: 4050/19876, Loss: 7.85, LR: 0.10\n",
      "Batch: 4100/19876, Loss: 7.61, LR: 0.10\n",
      "Batch: 4150/19876, Loss: 7.63, LR: 0.10\n",
      "Batch: 4200/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 4250/19876, Loss: 8.14, LR: 0.10\n",
      "Batch: 4300/19876, Loss: 7.45, LR: 0.10\n",
      "Batch: 4350/19876, Loss: 7.76, LR: 0.10\n",
      "Batch: 4400/19876, Loss: 7.95, LR: 0.10\n",
      "Batch: 4450/19876, Loss: 8.31, LR: 0.10\n",
      "Batch: 4500/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 4550/19876, Loss: 8.30, LR: 0.10\n",
      "Batch: 4600/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 4650/19876, Loss: 8.30, LR: 0.10\n",
      "Batch: 4700/19876, Loss: 6.77, LR: 0.10\n",
      "Batch: 4750/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 4800/19876, Loss: 7.56, LR: 0.10\n",
      "Batch: 4850/19876, Loss: 8.20, LR: 0.10\n",
      "Batch: 4900/19876, Loss: 6.91, LR: 0.10\n",
      "Batch: 4950/19876, Loss: 7.75, LR: 0.10\n",
      "Batch: 5000/19876, Loss: 7.79, LR: 0.10\n",
      "Batch: 5050/19876, Loss: 7.46, LR: 0.10\n",
      "Batch: 5100/19876, Loss: 7.74, LR: 0.10\n",
      "Batch: 5150/19876, Loss: 7.67, LR: 0.10\n",
      "Batch: 5200/19876, Loss: 7.38, LR: 0.10\n",
      "Batch: 5250/19876, Loss: 7.47, LR: 0.10\n",
      "Batch: 5300/19876, Loss: 7.48, LR: 0.10\n",
      "Batch: 5350/19876, Loss: 7.45, LR: 0.10\n",
      "Batch: 5400/19876, Loss: 8.13, LR: 0.10\n",
      "Batch: 5450/19876, Loss: 7.63, LR: 0.10\n",
      "Batch: 5500/19876, Loss: 7.74, LR: 0.10\n",
      "Batch: 5550/19876, Loss: 7.08, LR: 0.10\n",
      "Batch: 5600/19876, Loss: 7.90, LR: 0.10\n",
      "Batch: 5650/19876, Loss: 7.56, LR: 0.10\n",
      "Batch: 5700/19876, Loss: 7.72, LR: 0.10\n",
      "Batch: 5750/19876, Loss: 7.77, LR: 0.10\n",
      "Batch: 5800/19876, Loss: 6.99, LR: 0.10\n",
      "Batch: 5850/19876, Loss: 8.28, LR: 0.10\n",
      "Batch: 5900/19876, Loss: 7.12, LR: 0.10\n",
      "Batch: 5950/19876, Loss: 7.56, LR: 0.10\n",
      "Batch: 6000/19876, Loss: 7.74, LR: 0.10\n",
      "Batch: 6050/19876, Loss: 7.70, LR: 0.10\n",
      "Batch: 6100/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 6150/19876, Loss: 7.84, LR: 0.10\n",
      "Batch: 6200/19876, Loss: 7.68, LR: 0.10\n",
      "Batch: 6250/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 6300/19876, Loss: 8.16, LR: 0.10\n",
      "Batch: 6350/19876, Loss: 8.08, LR: 0.10\n",
      "Batch: 6400/19876, Loss: 7.62, LR: 0.10\n",
      "Batch: 6450/19876, Loss: 7.74, LR: 0.10\n",
      "Batch: 6500/19876, Loss: 7.78, LR: 0.10\n",
      "Batch: 6550/19876, Loss: 7.82, LR: 0.10\n",
      "Batch: 6600/19876, Loss: 7.84, LR: 0.10\n",
      "Batch: 6650/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 6700/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 6750/19876, Loss: 7.26, LR: 0.10\n",
      "Batch: 6800/19876, Loss: 7.52, LR: 0.10\n",
      "Batch: 6850/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 6900/19876, Loss: 7.17, LR: 0.10\n",
      "Batch: 6950/19876, Loss: 7.21, LR: 0.10\n",
      "Batch: 7000/19876, Loss: 7.20, LR: 0.10\n",
      "Batch: 7050/19876, Loss: 7.24, LR: 0.10\n",
      "Batch: 7100/19876, Loss: 8.06, LR: 0.10\n",
      "Batch: 7150/19876, Loss: 7.81, LR: 0.10\n",
      "Batch: 7200/19876, Loss: 7.62, LR: 0.10\n",
      "Batch: 7250/19876, Loss: 7.71, LR: 0.10\n",
      "Batch: 7300/19876, Loss: 7.38, LR: 0.10\n",
      "Batch: 7350/19876, Loss: 7.47, LR: 0.10\n",
      "Batch: 7400/19876, Loss: 6.88, LR: 0.10\n",
      "Batch: 7450/19876, Loss: 7.83, LR: 0.10\n",
      "Batch: 7500/19876, Loss: 7.58, LR: 0.10\n",
      "Batch: 7550/19876, Loss: 7.20, LR: 0.10\n",
      "Batch: 7600/19876, Loss: 7.78, LR: 0.10\n",
      "Batch: 7650/19876, Loss: 7.07, LR: 0.10\n",
      "Batch: 7700/19876, Loss: 7.80, LR: 0.10\n",
      "Batch: 7750/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 7800/19876, Loss: 7.55, LR: 0.10\n",
      "Batch: 7850/19876, Loss: 7.26, LR: 0.10\n",
      "Batch: 7900/19876, Loss: 7.67, LR: 0.10\n",
      "Batch: 7950/19876, Loss: 8.00, LR: 0.10\n",
      "Batch: 8000/19876, Loss: 7.62, LR: 0.10\n",
      "Batch: 8050/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 8100/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 8150/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 8200/19876, Loss: 7.34, LR: 0.10\n",
      "Batch: 8250/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 8300/19876, Loss: 7.29, LR: 0.10\n",
      "Batch: 8350/19876, Loss: 7.37, LR: 0.10\n",
      "Batch: 8400/19876, Loss: 7.51, LR: 0.10\n",
      "Batch: 8450/19876, Loss: 7.53, LR: 0.10\n",
      "Batch: 8500/19876, Loss: 7.54, LR: 0.10\n",
      "Batch: 8550/19876, Loss: 6.94, LR: 0.10\n",
      "Batch: 8600/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 8650/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 8700/19876, Loss: 7.65, LR: 0.10\n",
      "Batch: 8750/19876, Loss: 6.97, LR: 0.10\n",
      "Batch: 8800/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 8850/19876, Loss: 7.33, LR: 0.10\n",
      "Batch: 8900/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 8950/19876, Loss: 7.52, LR: 0.10\n",
      "Batch: 9000/19876, Loss: 7.38, LR: 0.10\n",
      "Batch: 9050/19876, Loss: 7.64, LR: 0.10\n",
      "Batch: 9100/19876, Loss: 7.03, LR: 0.10\n",
      "Batch: 9150/19876, Loss: 7.34, LR: 0.10\n",
      "Batch: 9200/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 9250/19876, Loss: 7.67, LR: 0.10\n",
      "Batch: 9300/19876, Loss: 7.43, LR: 0.10\n",
      "Batch: 9350/19876, Loss: 7.00, LR: 0.10\n",
      "Batch: 9400/19876, Loss: 7.33, LR: 0.10\n",
      "Batch: 9450/19876, Loss: 7.55, LR: 0.10\n",
      "Batch: 9500/19876, Loss: 7.63, LR: 0.10\n",
      "Batch: 9550/19876, Loss: 6.78, LR: 0.10\n",
      "Batch: 9600/19876, Loss: 7.48, LR: 0.10\n",
      "Batch: 9650/19876, Loss: 7.95, LR: 0.10\n",
      "Batch: 9700/19876, Loss: 7.54, LR: 0.10\n",
      "Batch: 9750/19876, Loss: 7.87, LR: 0.10\n",
      "Batch: 9800/19876, Loss: 6.96, LR: 0.10\n",
      "Batch: 9850/19876, Loss: 7.37, LR: 0.10\n",
      "Batch: 9900/19876, Loss: 7.34, LR: 0.10\n",
      "Batch: 9950/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 10000/19876, Loss: 7.20, LR: 0.10\n",
      "Batch: 10050/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 10100/19876, Loss: 7.49, LR: 0.10\n",
      "Batch: 10150/19876, Loss: 7.71, LR: 0.10\n",
      "Batch: 10200/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 10250/19876, Loss: 6.85, LR: 0.10\n",
      "Batch: 10300/19876, Loss: 7.37, LR: 0.10\n",
      "Batch: 10350/19876, Loss: 7.43, LR: 0.10\n",
      "Batch: 10400/19876, Loss: 6.99, LR: 0.10\n",
      "Batch: 10450/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 10500/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 10550/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 10600/19876, Loss: 7.10, LR: 0.10\n",
      "Batch: 10650/19876, Loss: 7.17, LR: 0.10\n",
      "Batch: 10700/19876, Loss: 7.85, LR: 0.10\n",
      "Batch: 10750/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 10800/19876, Loss: 6.94, LR: 0.10\n",
      "Batch: 10850/19876, Loss: 7.29, LR: 0.10\n",
      "Batch: 10900/19876, Loss: 7.92, LR: 0.10\n",
      "Batch: 10950/19876, Loss: 7.12, LR: 0.10\n",
      "Batch: 11000/19876, Loss: 7.36, LR: 0.10\n",
      "Batch: 11050/19876, Loss: 7.39, LR: 0.10\n",
      "Batch: 11100/19876, Loss: 7.11, LR: 0.10\n",
      "Batch: 11150/19876, Loss: 7.16, LR: 0.10\n",
      "Batch: 11200/19876, Loss: 7.04, LR: 0.10\n",
      "Batch: 11250/19876, Loss: 7.40, LR: 0.10\n",
      "Batch: 11300/19876, Loss: 7.90, LR: 0.10\n",
      "Batch: 11350/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 11400/19876, Loss: 7.20, LR: 0.10\n",
      "Batch: 11450/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 11500/19876, Loss: 7.49, LR: 0.10\n",
      "Batch: 11550/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 11600/19876, Loss: 7.22, LR: 0.10\n",
      "Batch: 11650/19876, Loss: 7.56, LR: 0.10\n",
      "Batch: 11700/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 11750/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 11800/19876, Loss: 8.00, LR: 0.10\n",
      "Batch: 11850/19876, Loss: 7.13, LR: 0.10\n",
      "Batch: 11900/19876, Loss: 6.99, LR: 0.10\n",
      "Batch: 11950/19876, Loss: 7.21, LR: 0.10\n",
      "Batch: 12000/19876, Loss: 6.99, LR: 0.10\n",
      "Batch: 12050/19876, Loss: 6.94, LR: 0.10\n",
      "Batch: 12100/19876, Loss: 6.79, LR: 0.10\n",
      "Batch: 12150/19876, Loss: 7.19, LR: 0.10\n",
      "Batch: 12200/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 12250/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 12300/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 12350/19876, Loss: 6.91, LR: 0.10\n",
      "Batch: 12400/19876, Loss: 7.47, LR: 0.10\n",
      "Batch: 12450/19876, Loss: 7.40, LR: 0.10\n",
      "Batch: 12500/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 12550/19876, Loss: 7.92, LR: 0.10\n",
      "Batch: 12600/19876, Loss: 7.24, LR: 0.10\n",
      "Batch: 12650/19876, Loss: 7.13, LR: 0.10\n",
      "Batch: 12700/19876, Loss: 7.23, LR: 0.10\n",
      "Batch: 12750/19876, Loss: 7.31, LR: 0.10\n",
      "Batch: 12800/19876, Loss: 7.21, LR: 0.10\n",
      "Batch: 12850/19876, Loss: 6.86, LR: 0.10\n",
      "Batch: 12900/19876, Loss: 7.28, LR: 0.10\n",
      "Batch: 12950/19876, Loss: 7.73, LR: 0.10\n",
      "Batch: 13000/19876, Loss: 7.61, LR: 0.10\n",
      "Batch: 13050/19876, Loss: 7.68, LR: 0.10\n",
      "Batch: 13100/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 13150/19876, Loss: 7.09, LR: 0.10\n",
      "Batch: 13200/19876, Loss: 7.14, LR: 0.10\n",
      "Batch: 13250/19876, Loss: 7.10, LR: 0.10\n",
      "Batch: 13300/19876, Loss: 7.69, LR: 0.10\n",
      "Batch: 13350/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 13400/19876, Loss: 7.22, LR: 0.10\n",
      "Batch: 13450/19876, Loss: 8.15, LR: 0.10\n",
      "Batch: 13500/19876, Loss: 7.36, LR: 0.10\n",
      "Batch: 13550/19876, Loss: 7.00, LR: 0.10\n",
      "Batch: 13600/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 13650/19876, Loss: 7.60, LR: 0.10\n",
      "Batch: 13700/19876, Loss: 7.70, LR: 0.10\n",
      "Batch: 13750/19876, Loss: 7.03, LR: 0.10\n",
      "Batch: 13800/19876, Loss: 7.30, LR: 0.10\n",
      "Batch: 13850/19876, Loss: 6.69, LR: 0.10\n",
      "Batch: 13900/19876, Loss: 8.04, LR: 0.10\n",
      "Batch: 13950/19876, Loss: 6.89, LR: 0.10\n",
      "Batch: 14000/19876, Loss: 7.62, LR: 0.10\n",
      "Batch: 14050/19876, Loss: 7.22, LR: 0.10\n",
      "Batch: 14100/19876, Loss: 7.16, LR: 0.10\n",
      "Batch: 14150/19876, Loss: 6.87, LR: 0.10\n",
      "Batch: 14200/19876, Loss: 7.80, LR: 0.10\n",
      "Batch: 14250/19876, Loss: 7.26, LR: 0.10\n",
      "Batch: 14300/19876, Loss: 7.59, LR: 0.10\n",
      "Batch: 14350/19876, Loss: 7.54, LR: 0.10\n",
      "Batch: 14400/19876, Loss: 6.77, LR: 0.10\n",
      "Batch: 14450/19876, Loss: 7.31, LR: 0.10\n",
      "Batch: 14500/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 14550/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 14600/19876, Loss: 7.04, LR: 0.10\n",
      "Batch: 14650/19876, Loss: 8.02, LR: 0.10\n",
      "Batch: 14700/19876, Loss: 7.57, LR: 0.10\n",
      "Batch: 14750/19876, Loss: 7.75, LR: 0.10\n",
      "Batch: 14800/19876, Loss: 7.58, LR: 0.10\n",
      "Batch: 14850/19876, Loss: 6.72, LR: 0.10\n",
      "Batch: 14900/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 14950/19876, Loss: 6.75, LR: 0.10\n",
      "Batch: 15000/19876, Loss: 7.24, LR: 0.10\n",
      "Batch: 15050/19876, Loss: 6.66, LR: 0.10\n",
      "Batch: 15100/19876, Loss: 6.67, LR: 0.10\n",
      "Batch: 15150/19876, Loss: 7.63, LR: 0.10\n",
      "Batch: 15200/19876, Loss: 7.51, LR: 0.10\n",
      "Batch: 15250/19876, Loss: 6.87, LR: 0.10\n",
      "Batch: 15300/19876, Loss: 7.02, LR: 0.10\n",
      "Batch: 15350/19876, Loss: 7.09, LR: 0.10\n",
      "Batch: 15400/19876, Loss: 6.96, LR: 0.10\n",
      "Batch: 15450/19876, Loss: 7.07, LR: 0.10\n",
      "Batch: 15500/19876, Loss: 6.85, LR: 0.10\n",
      "Batch: 15550/19876, Loss: 7.13, LR: 0.10\n",
      "Batch: 15600/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 15650/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 15700/19876, Loss: 7.34, LR: 0.10\n",
      "Batch: 15750/19876, Loss: 7.08, LR: 0.10\n",
      "Batch: 15800/19876, Loss: 7.15, LR: 0.10\n",
      "Batch: 15850/19876, Loss: 7.58, LR: 0.10\n",
      "Batch: 15900/19876, Loss: 6.83, LR: 0.10\n",
      "Batch: 15950/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 16000/19876, Loss: 7.10, LR: 0.10\n",
      "Batch: 16050/19876, Loss: 7.50, LR: 0.10\n",
      "Batch: 16100/19876, Loss: 7.16, LR: 0.10\n",
      "Batch: 16150/19876, Loss: 7.33, LR: 0.10\n",
      "Batch: 16200/19876, Loss: 7.11, LR: 0.10\n",
      "Batch: 16250/19876, Loss: 7.07, LR: 0.10\n",
      "Batch: 16300/19876, Loss: 7.02, LR: 0.10\n",
      "Batch: 16350/19876, Loss: 7.68, LR: 0.10\n",
      "Batch: 16400/19876, Loss: 7.08, LR: 0.10\n",
      "Batch: 16450/19876, Loss: 7.64, LR: 0.10\n",
      "Batch: 16500/19876, Loss: 7.21, LR: 0.10\n",
      "Batch: 16550/19876, Loss: 7.03, LR: 0.10\n",
      "Batch: 16600/19876, Loss: 7.38, LR: 0.10\n",
      "Batch: 16650/19876, Loss: 7.14, LR: 0.10\n",
      "Batch: 16700/19876, Loss: 7.54, LR: 0.10\n",
      "Batch: 16750/19876, Loss: 6.91, LR: 0.10\n",
      "Batch: 16800/19876, Loss: 7.42, LR: 0.10\n",
      "Batch: 16850/19876, Loss: 7.24, LR: 0.10\n",
      "Batch: 16900/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 16950/19876, Loss: 7.02, LR: 0.10\n",
      "Batch: 17000/19876, Loss: 7.61, LR: 0.10\n",
      "Batch: 17050/19876, Loss: 7.67, LR: 0.10\n",
      "Batch: 17100/19876, Loss: 7.48, LR: 0.10\n",
      "Batch: 17150/19876, Loss: 7.19, LR: 0.10\n",
      "Batch: 17200/19876, Loss: 7.25, LR: 0.10\n",
      "Batch: 17250/19876, Loss: 6.79, LR: 0.10\n",
      "Batch: 17300/19876, Loss: 7.02, LR: 0.10\n",
      "Batch: 17350/19876, Loss: 7.64, LR: 0.10\n",
      "Batch: 17400/19876, Loss: 7.39, LR: 0.10\n",
      "Batch: 17450/19876, Loss: 7.19, LR: 0.10\n",
      "Batch: 17500/19876, Loss: 7.34, LR: 0.10\n",
      "Batch: 17550/19876, Loss: 7.23, LR: 0.10\n",
      "Batch: 17600/19876, Loss: 7.30, LR: 0.10\n",
      "Batch: 17650/19876, Loss: 7.03, LR: 0.10\n",
      "Batch: 17700/19876, Loss: 6.92, LR: 0.10\n",
      "Batch: 17750/19876, Loss: 7.08, LR: 0.10\n",
      "Batch: 17800/19876, Loss: 6.91, LR: 0.10\n",
      "Batch: 17850/19876, Loss: 7.08, LR: 0.10\n",
      "Batch: 17900/19876, Loss: 6.79, LR: 0.10\n",
      "Batch: 17950/19876, Loss: 7.36, LR: 0.10\n",
      "Batch: 18000/19876, Loss: 7.39, LR: 0.10\n",
      "Batch: 18050/19876, Loss: 7.17, LR: 0.10\n",
      "Batch: 18100/19876, Loss: 7.14, LR: 0.10\n",
      "Batch: 18150/19876, Loss: 7.27, LR: 0.10\n",
      "Batch: 18200/19876, Loss: 6.82, LR: 0.10\n",
      "Batch: 18250/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 18300/19876, Loss: 7.39, LR: 0.10\n",
      "Batch: 18350/19876, Loss: 6.76, LR: 0.10\n",
      "Batch: 18400/19876, Loss: 6.86, LR: 0.10\n",
      "Batch: 18450/19876, Loss: 7.97, LR: 0.10\n",
      "Batch: 18500/19876, Loss: 7.21, LR: 0.10\n",
      "Batch: 18550/19876, Loss: 7.80, LR: 0.10\n",
      "Batch: 18600/19876, Loss: 7.29, LR: 0.10\n",
      "Batch: 18650/19876, Loss: 7.17, LR: 0.10\n",
      "Batch: 18700/19876, Loss: 6.46, LR: 0.10\n",
      "Batch: 18750/19876, Loss: 6.89, LR: 0.10\n",
      "Batch: 18800/19876, Loss: 7.23, LR: 0.10\n",
      "Batch: 18850/19876, Loss: 6.89, LR: 0.10\n",
      "Batch: 18900/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 18950/19876, Loss: 6.56, LR: 0.10\n",
      "Batch: 19000/19876, Loss: 7.13, LR: 0.10\n",
      "Batch: 19050/19876, Loss: 6.66, LR: 0.10\n",
      "Batch: 19100/19876, Loss: 6.83, LR: 0.10\n",
      "Batch: 19150/19876, Loss: 7.12, LR: 0.10\n",
      "Batch: 19200/19876, Loss: 6.95, LR: 0.10\n",
      "Batch: 19250/19876, Loss: 6.91, LR: 0.10\n",
      "Batch: 19300/19876, Loss: 7.12, LR: 0.10\n",
      "Batch: 19350/19876, Loss: 7.48, LR: 0.10\n",
      "Batch: 19400/19876, Loss: 7.36, LR: 0.10\n",
      "Batch: 19450/19876, Loss: 6.92, LR: 0.10\n",
      "Batch: 19500/19876, Loss: 7.06, LR: 0.10\n",
      "Batch: 19550/19876, Loss: 7.23, LR: 0.10\n",
      "Batch: 19600/19876, Loss: 7.32, LR: 0.10\n",
      "Batch: 19650/19876, Loss: 7.35, LR: 0.10\n",
      "Batch: 19700/19876, Loss: 7.03, LR: 0.10\n",
      "Batch: 19750/19876, Loss: 6.87, LR: 0.10\n",
      "Batch: 19800/19876, Loss: 7.26, LR: 0.10\n",
      "Batch: 19850/19876, Loss: 7.05, LR: 0.10\n",
      "Batch: 0/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 50/19876, Loss: 7.18, LR: 0.05\n",
      "Batch: 100/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 150/19876, Loss: 7.37, LR: 0.05\n",
      "Batch: 200/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 250/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 300/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 350/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 400/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 450/19876, Loss: 7.81, LR: 0.05\n",
      "Batch: 500/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 550/19876, Loss: 7.01, LR: 0.05\n",
      "Batch: 600/19876, Loss: 7.16, LR: 0.05\n",
      "Batch: 650/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 700/19876, Loss: 7.36, LR: 0.05\n",
      "Batch: 750/19876, Loss: 6.60, LR: 0.05\n",
      "Batch: 800/19876, Loss: 7.75, LR: 0.05\n",
      "Batch: 850/19876, Loss: 6.87, LR: 0.05\n",
      "Batch: 900/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 950/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 1000/19876, Loss: 7.11, LR: 0.05\n",
      "Batch: 1050/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 1100/19876, Loss: 6.76, LR: 0.05\n",
      "Batch: 1150/19876, Loss: 7.91, LR: 0.05\n",
      "Batch: 1200/19876, Loss: 6.92, LR: 0.05\n",
      "Batch: 1250/19876, Loss: 7.99, LR: 0.05\n",
      "Batch: 1300/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 1350/19876, Loss: 7.08, LR: 0.05\n",
      "Batch: 1400/19876, Loss: 7.56, LR: 0.05\n",
      "Batch: 1450/19876, Loss: 7.46, LR: 0.05\n",
      "Batch: 1500/19876, Loss: 7.29, LR: 0.05\n",
      "Batch: 1550/19876, Loss: 7.37, LR: 0.05\n",
      "Batch: 1600/19876, Loss: 7.64, LR: 0.05\n",
      "Batch: 1650/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 1700/19876, Loss: 7.43, LR: 0.05\n",
      "Batch: 1750/19876, Loss: 6.90, LR: 0.05\n",
      "Batch: 1800/19876, Loss: 7.46, LR: 0.05\n",
      "Batch: 1850/19876, Loss: 7.34, LR: 0.05\n",
      "Batch: 1900/19876, Loss: 7.22, LR: 0.05\n",
      "Batch: 1950/19876, Loss: 7.35, LR: 0.05\n",
      "Batch: 2000/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 2050/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 2100/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 2150/19876, Loss: 7.34, LR: 0.05\n",
      "Batch: 2200/19876, Loss: 7.18, LR: 0.05\n",
      "Batch: 2250/19876, Loss: 7.30, LR: 0.05\n",
      "Batch: 2300/19876, Loss: 7.59, LR: 0.05\n",
      "Batch: 2350/19876, Loss: 6.73, LR: 0.05\n",
      "Batch: 2400/19876, Loss: 7.53, LR: 0.05\n",
      "Batch: 2450/19876, Loss: 7.62, LR: 0.05\n",
      "Batch: 2500/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 2550/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 2600/19876, Loss: 7.68, LR: 0.05\n",
      "Batch: 2650/19876, Loss: 7.43, LR: 0.05\n",
      "Batch: 2700/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 2750/19876, Loss: 7.20, LR: 0.05\n",
      "Batch: 2800/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 2850/19876, Loss: 7.65, LR: 0.05\n",
      "Batch: 2900/19876, Loss: 6.97, LR: 0.05\n",
      "Batch: 2950/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 3000/19876, Loss: 6.90, LR: 0.05\n",
      "Batch: 3050/19876, Loss: 6.36, LR: 0.05\n",
      "Batch: 3100/19876, Loss: 7.41, LR: 0.05\n",
      "Batch: 3150/19876, Loss: 6.89, LR: 0.05\n",
      "Batch: 3200/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 3250/19876, Loss: 7.89, LR: 0.05\n",
      "Batch: 3300/19876, Loss: 6.77, LR: 0.05\n",
      "Batch: 3350/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 3400/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 3450/19876, Loss: 6.71, LR: 0.05\n",
      "Batch: 3500/19876, Loss: 7.70, LR: 0.05\n",
      "Batch: 3550/19876, Loss: 7.30, LR: 0.05\n",
      "Batch: 3600/19876, Loss: 7.45, LR: 0.05\n",
      "Batch: 3650/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 3700/19876, Loss: 7.02, LR: 0.05\n",
      "Batch: 3750/19876, Loss: 6.97, LR: 0.05\n",
      "Batch: 3800/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 3850/19876, Loss: 7.21, LR: 0.05\n",
      "Batch: 3900/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 3950/19876, Loss: 7.49, LR: 0.05\n",
      "Batch: 4000/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 4050/19876, Loss: 7.07, LR: 0.05\n",
      "Batch: 4100/19876, Loss: 7.43, LR: 0.05\n",
      "Batch: 4150/19876, Loss: 6.89, LR: 0.05\n",
      "Batch: 4200/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 4250/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 4300/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 4350/19876, Loss: 6.90, LR: 0.05\n",
      "Batch: 4400/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 4450/19876, Loss: 7.26, LR: 0.05\n",
      "Batch: 4500/19876, Loss: 6.81, LR: 0.05\n",
      "Batch: 4550/19876, Loss: 7.16, LR: 0.05\n",
      "Batch: 4600/19876, Loss: 7.13, LR: 0.05\n",
      "Batch: 4650/19876, Loss: 7.26, LR: 0.05\n",
      "Batch: 4700/19876, Loss: 6.89, LR: 0.05\n",
      "Batch: 4750/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 4800/19876, Loss: 7.29, LR: 0.05\n",
      "Batch: 4850/19876, Loss: 6.97, LR: 0.05\n",
      "Batch: 4900/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 4950/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 5000/19876, Loss: 6.73, LR: 0.05\n",
      "Batch: 5050/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 5100/19876, Loss: 7.36, LR: 0.05\n",
      "Batch: 5150/19876, Loss: 7.37, LR: 0.05\n",
      "Batch: 5200/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 5250/19876, Loss: 6.75, LR: 0.05\n",
      "Batch: 5300/19876, Loss: 6.87, LR: 0.05\n",
      "Batch: 5350/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 5400/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 5450/19876, Loss: 7.21, LR: 0.05\n",
      "Batch: 5500/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 5550/19876, Loss: 7.38, LR: 0.05\n",
      "Batch: 5600/19876, Loss: 6.34, LR: 0.05\n",
      "Batch: 5650/19876, Loss: 6.84, LR: 0.05\n",
      "Batch: 5700/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 5750/19876, Loss: 7.26, LR: 0.05\n",
      "Batch: 5800/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 5850/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 5900/19876, Loss: 6.85, LR: 0.05\n",
      "Batch: 5950/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 6000/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 6050/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 6100/19876, Loss: 7.12, LR: 0.05\n",
      "Batch: 6150/19876, Loss: 7.22, LR: 0.05\n",
      "Batch: 6200/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 6250/19876, Loss: 7.11, LR: 0.05\n",
      "Batch: 6300/19876, Loss: 6.72, LR: 0.05\n",
      "Batch: 6350/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 6400/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 6450/19876, Loss: 6.68, LR: 0.05\n",
      "Batch: 6500/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 6550/19876, Loss: 7.29, LR: 0.05\n",
      "Batch: 6600/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 6650/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 6700/19876, Loss: 7.04, LR: 0.05\n",
      "Batch: 6750/19876, Loss: 7.34, LR: 0.05\n",
      "Batch: 6800/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 6850/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 6900/19876, Loss: 6.90, LR: 0.05\n",
      "Batch: 6950/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 7000/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 7050/19876, Loss: 7.49, LR: 0.05\n",
      "Batch: 7100/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 7150/19876, Loss: 7.45, LR: 0.05\n",
      "Batch: 7200/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 7250/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 7300/19876, Loss: 7.17, LR: 0.05\n",
      "Batch: 7350/19876, Loss: 6.82, LR: 0.05\n",
      "Batch: 7400/19876, Loss: 6.88, LR: 0.05\n",
      "Batch: 7450/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 7500/19876, Loss: 7.44, LR: 0.05\n",
      "Batch: 7550/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 7600/19876, Loss: 7.40, LR: 0.05\n",
      "Batch: 7650/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 7700/19876, Loss: 7.42, LR: 0.05\n",
      "Batch: 7750/19876, Loss: 7.01, LR: 0.05\n",
      "Batch: 7800/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 7850/19876, Loss: 7.35, LR: 0.05\n",
      "Batch: 7900/19876, Loss: 6.61, LR: 0.05\n",
      "Batch: 7950/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 8000/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 8050/19876, Loss: 6.85, LR: 0.05\n",
      "Batch: 8100/19876, Loss: 7.62, LR: 0.05\n",
      "Batch: 8150/19876, Loss: 7.35, LR: 0.05\n",
      "Batch: 8200/19876, Loss: 7.18, LR: 0.05\n",
      "Batch: 8250/19876, Loss: 6.82, LR: 0.05\n",
      "Batch: 8300/19876, Loss: 6.46, LR: 0.05\n",
      "Batch: 8350/19876, Loss: 6.68, LR: 0.05\n",
      "Batch: 8400/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 8450/19876, Loss: 7.16, LR: 0.05\n",
      "Batch: 8500/19876, Loss: 6.85, LR: 0.05\n",
      "Batch: 8550/19876, Loss: 7.68, LR: 0.05\n",
      "Batch: 8600/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 8650/19876, Loss: 7.54, LR: 0.05\n",
      "Batch: 8700/19876, Loss: 7.44, LR: 0.05\n",
      "Batch: 8750/19876, Loss: 6.67, LR: 0.05\n",
      "Batch: 8800/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 8850/19876, Loss: 6.63, LR: 0.05\n",
      "Batch: 8900/19876, Loss: 7.41, LR: 0.05\n",
      "Batch: 8950/19876, Loss: 7.59, LR: 0.05\n",
      "Batch: 9000/19876, Loss: 7.70, LR: 0.05\n",
      "Batch: 9050/19876, Loss: 7.16, LR: 0.05\n",
      "Batch: 9100/19876, Loss: 6.84, LR: 0.05\n",
      "Batch: 9150/19876, Loss: 7.76, LR: 0.05\n",
      "Batch: 9200/19876, Loss: 7.45, LR: 0.05\n",
      "Batch: 9250/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 9300/19876, Loss: 7.79, LR: 0.05\n",
      "Batch: 9350/19876, Loss: 7.29, LR: 0.05\n",
      "Batch: 9400/19876, Loss: 7.45, LR: 0.05\n",
      "Batch: 9450/19876, Loss: 7.01, LR: 0.05\n",
      "Batch: 9500/19876, Loss: 6.75, LR: 0.05\n",
      "Batch: 9550/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 9600/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 9650/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 9700/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 9750/19876, Loss: 7.44, LR: 0.05\n",
      "Batch: 9800/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 9850/19876, Loss: 6.86, LR: 0.05\n",
      "Batch: 9900/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 9950/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 10000/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 10050/19876, Loss: 7.21, LR: 0.05\n",
      "Batch: 10100/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 10150/19876, Loss: 6.60, LR: 0.05\n",
      "Batch: 10200/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 10250/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 10300/19876, Loss: 6.62, LR: 0.05\n",
      "Batch: 10350/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 10400/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 10450/19876, Loss: 6.88, LR: 0.05\n",
      "Batch: 10500/19876, Loss: 7.48, LR: 0.05\n",
      "Batch: 10550/19876, Loss: 7.47, LR: 0.05\n",
      "Batch: 10600/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 10650/19876, Loss: 7.53, LR: 0.05\n",
      "Batch: 10700/19876, Loss: 6.70, LR: 0.05\n",
      "Batch: 10750/19876, Loss: 7.03, LR: 0.05\n",
      "Batch: 10800/19876, Loss: 7.19, LR: 0.05\n",
      "Batch: 10850/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 10900/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 10950/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 11000/19876, Loss: 7.52, LR: 0.05\n",
      "Batch: 11050/19876, Loss: 7.50, LR: 0.05\n",
      "Batch: 11100/19876, Loss: 7.34, LR: 0.05\n",
      "Batch: 11150/19876, Loss: 6.77, LR: 0.05\n",
      "Batch: 11200/19876, Loss: 7.54, LR: 0.05\n",
      "Batch: 11250/19876, Loss: 7.03, LR: 0.05\n",
      "Batch: 11300/19876, Loss: 6.92, LR: 0.05\n",
      "Batch: 11350/19876, Loss: 7.00, LR: 0.05\n",
      "Batch: 11400/19876, Loss: 7.13, LR: 0.05\n",
      "Batch: 11450/19876, Loss: 7.01, LR: 0.05\n",
      "Batch: 11500/19876, Loss: 6.68, LR: 0.05\n",
      "Batch: 11550/19876, Loss: 7.26, LR: 0.05\n",
      "Batch: 11600/19876, Loss: 7.39, LR: 0.05\n",
      "Batch: 11650/19876, Loss: 7.00, LR: 0.05\n",
      "Batch: 11700/19876, Loss: 6.56, LR: 0.05\n",
      "Batch: 11750/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 11800/19876, Loss: 7.17, LR: 0.05\n",
      "Batch: 11850/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 11900/19876, Loss: 6.88, LR: 0.05\n",
      "Batch: 11950/19876, Loss: 6.67, LR: 0.05\n",
      "Batch: 12000/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 12050/19876, Loss: 7.00, LR: 0.05\n",
      "Batch: 12100/19876, Loss: 6.73, LR: 0.05\n",
      "Batch: 12150/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 12200/19876, Loss: 7.32, LR: 0.05\n",
      "Batch: 12250/19876, Loss: 6.54, LR: 0.05\n",
      "Batch: 12300/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 12350/19876, Loss: 7.40, LR: 0.05\n",
      "Batch: 12400/19876, Loss: 6.62, LR: 0.05\n",
      "Batch: 12450/19876, Loss: 6.59, LR: 0.05\n",
      "Batch: 12500/19876, Loss: 7.11, LR: 0.05\n",
      "Batch: 12550/19876, Loss: 6.53, LR: 0.05\n",
      "Batch: 12600/19876, Loss: 6.97, LR: 0.05\n",
      "Batch: 12650/19876, Loss: 6.72, LR: 0.05\n",
      "Batch: 12700/19876, Loss: 6.61, LR: 0.05\n",
      "Batch: 12750/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 12800/19876, Loss: 7.00, LR: 0.05\n",
      "Batch: 12850/19876, Loss: 6.63, LR: 0.05\n",
      "Batch: 12900/19876, Loss: 7.44, LR: 0.05\n",
      "Batch: 12950/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 13000/19876, Loss: 6.68, LR: 0.05\n",
      "Batch: 13050/19876, Loss: 7.53, LR: 0.05\n",
      "Batch: 13100/19876, Loss: 6.71, LR: 0.05\n",
      "Batch: 13150/19876, Loss: 7.04, LR: 0.05\n",
      "Batch: 13200/19876, Loss: 6.85, LR: 0.05\n",
      "Batch: 13250/19876, Loss: 6.86, LR: 0.05\n",
      "Batch: 13300/19876, Loss: 7.46, LR: 0.05\n",
      "Batch: 13350/19876, Loss: 7.00, LR: 0.05\n",
      "Batch: 13400/19876, Loss: 6.68, LR: 0.05\n",
      "Batch: 13450/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 13500/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 13550/19876, Loss: 6.98, LR: 0.05\n",
      "Batch: 13600/19876, Loss: 7.31, LR: 0.05\n",
      "Batch: 13650/19876, Loss: 6.70, LR: 0.05\n",
      "Batch: 13700/19876, Loss: 6.84, LR: 0.05\n",
      "Batch: 13750/19876, Loss: 7.16, LR: 0.05\n",
      "Batch: 13800/19876, Loss: 7.30, LR: 0.05\n",
      "Batch: 13850/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 13900/19876, Loss: 6.66, LR: 0.05\n",
      "Batch: 13950/19876, Loss: 6.84, LR: 0.05\n",
      "Batch: 14000/19876, Loss: 6.95, LR: 0.05\n",
      "Batch: 14050/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 14100/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 14150/19876, Loss: 6.60, LR: 0.05\n",
      "Batch: 14200/19876, Loss: 6.97, LR: 0.05\n",
      "Batch: 14250/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 14300/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 14350/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 14400/19876, Loss: 7.14, LR: 0.05\n",
      "Batch: 14450/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 14500/19876, Loss: 6.82, LR: 0.05\n",
      "Batch: 14550/19876, Loss: 7.37, LR: 0.05\n",
      "Batch: 14600/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 14650/19876, Loss: 6.57, LR: 0.05\n",
      "Batch: 14700/19876, Loss: 7.12, LR: 0.05\n",
      "Batch: 14750/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 14800/19876, Loss: 6.93, LR: 0.05\n",
      "Batch: 14850/19876, Loss: 7.37, LR: 0.05\n",
      "Batch: 14900/19876, Loss: 7.12, LR: 0.05\n",
      "Batch: 14950/19876, Loss: 6.79, LR: 0.05\n",
      "Batch: 15000/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 15050/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 15100/19876, Loss: 6.61, LR: 0.05\n",
      "Batch: 15150/19876, Loss: 6.86, LR: 0.05\n",
      "Batch: 15200/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 15250/19876, Loss: 7.02, LR: 0.05\n",
      "Batch: 15300/19876, Loss: 7.15, LR: 0.05\n",
      "Batch: 15350/19876, Loss: 7.27, LR: 0.05\n",
      "Batch: 15400/19876, Loss: 6.71, LR: 0.05\n",
      "Batch: 15450/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 15500/19876, Loss: 6.83, LR: 0.05\n",
      "Batch: 15550/19876, Loss: 6.43, LR: 0.05\n",
      "Batch: 15600/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 15650/19876, Loss: 6.75, LR: 0.05\n",
      "Batch: 15700/19876, Loss: 6.82, LR: 0.05\n",
      "Batch: 15750/19876, Loss: 7.49, LR: 0.05\n",
      "Batch: 15800/19876, Loss: 7.62, LR: 0.05\n",
      "Batch: 15850/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 15900/19876, Loss: 7.25, LR: 0.05\n",
      "Batch: 15950/19876, Loss: 7.01, LR: 0.05\n",
      "Batch: 16000/19876, Loss: 6.89, LR: 0.05\n",
      "Batch: 16050/19876, Loss: 6.48, LR: 0.05\n",
      "Batch: 16100/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 16150/19876, Loss: 6.87, LR: 0.05\n",
      "Batch: 16200/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 16250/19876, Loss: 7.11, LR: 0.05\n",
      "Batch: 16300/19876, Loss: 7.12, LR: 0.05\n",
      "Batch: 16350/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 16400/19876, Loss: 6.72, LR: 0.05\n",
      "Batch: 16450/19876, Loss: 6.52, LR: 0.05\n",
      "Batch: 16500/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 16550/19876, Loss: 7.08, LR: 0.05\n",
      "Batch: 16600/19876, Loss: 6.96, LR: 0.05\n",
      "Batch: 16650/19876, Loss: 7.40, LR: 0.05\n",
      "Batch: 16700/19876, Loss: 7.09, LR: 0.05\n",
      "Batch: 16750/19876, Loss: 7.17, LR: 0.05\n",
      "Batch: 16800/19876, Loss: 7.20, LR: 0.05\n",
      "Batch: 16850/19876, Loss: 7.23, LR: 0.05\n",
      "Batch: 16900/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 16950/19876, Loss: 6.94, LR: 0.05\n",
      "Batch: 17000/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 17050/19876, Loss: 7.22, LR: 0.05\n",
      "Batch: 17100/19876, Loss: 7.43, LR: 0.05\n",
      "Batch: 17150/19876, Loss: 6.93, LR: 0.05\n",
      "Batch: 17200/19876, Loss: 7.29, LR: 0.05\n",
      "Batch: 17250/19876, Loss: 7.32, LR: 0.05\n",
      "Batch: 17300/19876, Loss: 7.04, LR: 0.05\n",
      "Batch: 17350/19876, Loss: 6.46, LR: 0.05\n",
      "Batch: 17400/19876, Loss: 7.20, LR: 0.05\n",
      "Batch: 17450/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 17500/19876, Loss: 7.17, LR: 0.05\n",
      "Batch: 17550/19876, Loss: 7.33, LR: 0.05\n",
      "Batch: 17600/19876, Loss: 6.64, LR: 0.05\n",
      "Batch: 17650/19876, Loss: 7.13, LR: 0.05\n",
      "Batch: 17700/19876, Loss: 6.66, LR: 0.05\n",
      "Batch: 17750/19876, Loss: 6.80, LR: 0.05\n",
      "Batch: 17800/19876, Loss: 7.42, LR: 0.05\n",
      "Batch: 17850/19876, Loss: 7.08, LR: 0.05\n",
      "Batch: 17900/19876, Loss: 6.64, LR: 0.05\n",
      "Batch: 17950/19876, Loss: 6.72, LR: 0.05\n",
      "Batch: 18000/19876, Loss: 6.91, LR: 0.05\n",
      "Batch: 18050/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 18100/19876, Loss: 7.10, LR: 0.05\n",
      "Batch: 18150/19876, Loss: 6.88, LR: 0.05\n",
      "Batch: 18200/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 18250/19876, Loss: 7.03, LR: 0.05\n",
      "Batch: 18300/19876, Loss: 7.07, LR: 0.05\n",
      "Batch: 18350/19876, Loss: 6.78, LR: 0.05\n",
      "Batch: 18400/19876, Loss: 7.30, LR: 0.05\n",
      "Batch: 18450/19876, Loss: 7.05, LR: 0.05\n",
      "Batch: 18500/19876, Loss: 7.28, LR: 0.05\n",
      "Batch: 18550/19876, Loss: 7.20, LR: 0.05\n",
      "Batch: 18600/19876, Loss: 7.35, LR: 0.05\n",
      "Batch: 18650/19876, Loss: 6.70, LR: 0.05\n",
      "Batch: 18700/19876, Loss: 6.72, LR: 0.05\n",
      "Batch: 18750/19876, Loss: 6.90, LR: 0.05\n",
      "Batch: 18800/19876, Loss: 6.71, LR: 0.05\n",
      "Batch: 18850/19876, Loss: 7.06, LR: 0.05\n",
      "Batch: 18900/19876, Loss: 7.18, LR: 0.05\n",
      "Batch: 18950/19876, Loss: 7.15, LR: 0.05\n",
      "Batch: 19000/19876, Loss: 7.21, LR: 0.05\n",
      "Batch: 19050/19876, Loss: 7.11, LR: 0.05\n",
      "Batch: 19100/19876, Loss: 6.98, LR: 0.05\n",
      "Batch: 19150/19876, Loss: 7.08, LR: 0.05\n",
      "Batch: 19200/19876, Loss: 6.70, LR: 0.05\n",
      "Batch: 19250/19876, Loss: 6.85, LR: 0.05\n",
      "Batch: 19300/19876, Loss: 7.24, LR: 0.05\n",
      "Batch: 19350/19876, Loss: 6.81, LR: 0.05\n",
      "Batch: 19400/19876, Loss: 7.15, LR: 0.05\n",
      "Batch: 19450/19876, Loss: 6.73, LR: 0.05\n",
      "Batch: 19500/19876, Loss: 6.56, LR: 0.05\n",
      "Batch: 19550/19876, Loss: 7.85, LR: 0.05\n",
      "Batch: 19600/19876, Loss: 7.45, LR: 0.05\n",
      "Batch: 19650/19876, Loss: 6.99, LR: 0.05\n",
      "Batch: 19700/19876, Loss: 7.36, LR: 0.05\n",
      "Batch: 19750/19876, Loss: 6.92, LR: 0.05\n",
      "Batch: 19800/19876, Loss: 7.17, LR: 0.05\n",
      "Batch: 19850/19876, Loss: 7.36, LR: 0.05\n",
      "Batch: 0/19876, Loss: 7.61, LR: 0.03\n",
      "Batch: 50/19876, Loss: 7.47, LR: 0.03\n",
      "Batch: 100/19876, Loss: 6.55, LR: 0.03\n",
      "Batch: 150/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 200/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 250/19876, Loss: 7.18, LR: 0.03\n",
      "Batch: 300/19876, Loss: 7.40, LR: 0.03\n",
      "Batch: 350/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 400/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 450/19876, Loss: 7.47, LR: 0.03\n",
      "Batch: 500/19876, Loss: 7.18, LR: 0.03\n",
      "Batch: 550/19876, Loss: 7.30, LR: 0.03\n",
      "Batch: 600/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 650/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 700/19876, Loss: 6.63, LR: 0.03\n",
      "Batch: 750/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 800/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 850/19876, Loss: 6.70, LR: 0.03\n",
      "Batch: 900/19876, Loss: 7.21, LR: 0.03\n",
      "Batch: 950/19876, Loss: 7.03, LR: 0.03\n",
      "Batch: 1000/19876, Loss: 6.77, LR: 0.03\n",
      "Batch: 1050/19876, Loss: 7.34, LR: 0.03\n",
      "Batch: 1100/19876, Loss: 7.37, LR: 0.03\n",
      "Batch: 1150/19876, Loss: 6.58, LR: 0.03\n",
      "Batch: 1200/19876, Loss: 7.29, LR: 0.03\n",
      "Batch: 1250/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 1300/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 1350/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 1400/19876, Loss: 7.34, LR: 0.03\n",
      "Batch: 1450/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 1500/19876, Loss: 7.38, LR: 0.03\n",
      "Batch: 1550/19876, Loss: 6.89, LR: 0.03\n",
      "Batch: 1600/19876, Loss: 7.41, LR: 0.03\n",
      "Batch: 1650/19876, Loss: 7.15, LR: 0.03\n",
      "Batch: 1700/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 1750/19876, Loss: 6.64, LR: 0.03\n",
      "Batch: 1800/19876, Loss: 7.13, LR: 0.03\n",
      "Batch: 1850/19876, Loss: 6.84, LR: 0.03\n",
      "Batch: 1900/19876, Loss: 6.92, LR: 0.03\n",
      "Batch: 1950/19876, Loss: 6.83, LR: 0.03\n",
      "Batch: 2000/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 2050/19876, Loss: 6.97, LR: 0.03\n",
      "Batch: 2100/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 2150/19876, Loss: 7.31, LR: 0.03\n",
      "Batch: 2200/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 2250/19876, Loss: 6.93, LR: 0.03\n",
      "Batch: 2300/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 2350/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 2400/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 2450/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 2500/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 2550/19876, Loss: 6.26, LR: 0.03\n",
      "Batch: 2600/19876, Loss: 7.55, LR: 0.03\n",
      "Batch: 2650/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 2700/19876, Loss: 7.37, LR: 0.03\n",
      "Batch: 2750/19876, Loss: 7.36, LR: 0.03\n",
      "Batch: 2800/19876, Loss: 6.85, LR: 0.03\n",
      "Batch: 2850/19876, Loss: 7.12, LR: 0.03\n",
      "Batch: 2900/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 2950/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 3000/19876, Loss: 7.53, LR: 0.03\n",
      "Batch: 3050/19876, Loss: 6.65, LR: 0.03\n",
      "Batch: 3100/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 3150/19876, Loss: 6.97, LR: 0.03\n",
      "Batch: 3200/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 3250/19876, Loss: 6.94, LR: 0.03\n",
      "Batch: 3300/19876, Loss: 7.03, LR: 0.03\n",
      "Batch: 3350/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 3400/19876, Loss: 6.81, LR: 0.03\n",
      "Batch: 3450/19876, Loss: 6.53, LR: 0.03\n",
      "Batch: 3500/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 3550/19876, Loss: 7.09, LR: 0.03\n",
      "Batch: 3600/19876, Loss: 7.43, LR: 0.03\n",
      "Batch: 3650/19876, Loss: 7.43, LR: 0.03\n",
      "Batch: 3700/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 3750/19876, Loss: 7.15, LR: 0.03\n",
      "Batch: 3800/19876, Loss: 7.39, LR: 0.03\n",
      "Batch: 3850/19876, Loss: 7.30, LR: 0.03\n",
      "Batch: 3900/19876, Loss: 6.72, LR: 0.03\n",
      "Batch: 3950/19876, Loss: 7.10, LR: 0.03\n",
      "Batch: 4000/19876, Loss: 7.39, LR: 0.03\n",
      "Batch: 4050/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 4100/19876, Loss: 7.03, LR: 0.03\n",
      "Batch: 4150/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 4200/19876, Loss: 7.36, LR: 0.03\n",
      "Batch: 4250/19876, Loss: 6.89, LR: 0.03\n",
      "Batch: 4300/19876, Loss: 7.10, LR: 0.03\n",
      "Batch: 4350/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 4400/19876, Loss: 7.07, LR: 0.03\n",
      "Batch: 4450/19876, Loss: 7.16, LR: 0.03\n",
      "Batch: 4500/19876, Loss: 6.63, LR: 0.03\n",
      "Batch: 4550/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 4600/19876, Loss: 6.81, LR: 0.03\n",
      "Batch: 4650/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 4700/19876, Loss: 6.76, LR: 0.03\n",
      "Batch: 4750/19876, Loss: 7.15, LR: 0.03\n",
      "Batch: 4800/19876, Loss: 6.64, LR: 0.03\n",
      "Batch: 4850/19876, Loss: 7.32, LR: 0.03\n",
      "Batch: 4900/19876, Loss: 7.28, LR: 0.03\n",
      "Batch: 4950/19876, Loss: 6.94, LR: 0.03\n",
      "Batch: 5000/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 5050/19876, Loss: 6.47, LR: 0.03\n",
      "Batch: 5100/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 5150/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 5200/19876, Loss: 6.61, LR: 0.03\n",
      "Batch: 5250/19876, Loss: 7.43, LR: 0.03\n",
      "Batch: 5300/19876, Loss: 6.35, LR: 0.03\n",
      "Batch: 5350/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 5400/19876, Loss: 6.82, LR: 0.03\n",
      "Batch: 5450/19876, Loss: 7.09, LR: 0.03\n",
      "Batch: 5500/19876, Loss: 6.82, LR: 0.03\n",
      "Batch: 5550/19876, Loss: 6.63, LR: 0.03\n",
      "Batch: 5600/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 5650/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 5700/19876, Loss: 7.48, LR: 0.03\n",
      "Batch: 5750/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 5800/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 5850/19876, Loss: 7.55, LR: 0.03\n",
      "Batch: 5900/19876, Loss: 6.62, LR: 0.03\n",
      "Batch: 5950/19876, Loss: 7.11, LR: 0.03\n",
      "Batch: 6000/19876, Loss: 7.16, LR: 0.03\n",
      "Batch: 6050/19876, Loss: 7.65, LR: 0.03\n",
      "Batch: 6100/19876, Loss: 6.93, LR: 0.03\n",
      "Batch: 6150/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 6200/19876, Loss: 6.91, LR: 0.03\n",
      "Batch: 6250/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 6300/19876, Loss: 7.10, LR: 0.03\n",
      "Batch: 6350/19876, Loss: 7.22, LR: 0.03\n",
      "Batch: 6400/19876, Loss: 7.45, LR: 0.03\n",
      "Batch: 6450/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 6500/19876, Loss: 7.15, LR: 0.03\n",
      "Batch: 6550/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 6600/19876, Loss: 7.23, LR: 0.03\n",
      "Batch: 6650/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 6700/19876, Loss: 6.67, LR: 0.03\n",
      "Batch: 6750/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 6800/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 6850/19876, Loss: 7.15, LR: 0.03\n",
      "Batch: 6900/19876, Loss: 7.73, LR: 0.03\n",
      "Batch: 6950/19876, Loss: 6.83, LR: 0.03\n",
      "Batch: 7000/19876, Loss: 7.30, LR: 0.03\n",
      "Batch: 7050/19876, Loss: 6.76, LR: 0.03\n",
      "Batch: 7100/19876, Loss: 7.13, LR: 0.03\n",
      "Batch: 7150/19876, Loss: 6.65, LR: 0.03\n",
      "Batch: 7200/19876, Loss: 6.94, LR: 0.03\n",
      "Batch: 7250/19876, Loss: 7.31, LR: 0.03\n",
      "Batch: 7300/19876, Loss: 7.73, LR: 0.03\n",
      "Batch: 7350/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 7400/19876, Loss: 6.84, LR: 0.03\n",
      "Batch: 7450/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 7500/19876, Loss: 7.13, LR: 0.03\n",
      "Batch: 7550/19876, Loss: 7.23, LR: 0.03\n",
      "Batch: 7600/19876, Loss: 6.85, LR: 0.03\n",
      "Batch: 7650/19876, Loss: 6.59, LR: 0.03\n",
      "Batch: 7700/19876, Loss: 7.45, LR: 0.03\n",
      "Batch: 7750/19876, Loss: 7.08, LR: 0.03\n",
      "Batch: 7800/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 7850/19876, Loss: 7.38, LR: 0.03\n",
      "Batch: 7900/19876, Loss: 7.40, LR: 0.03\n",
      "Batch: 7950/19876, Loss: 6.76, LR: 0.03\n",
      "Batch: 8000/19876, Loss: 6.82, LR: 0.03\n",
      "Batch: 8050/19876, Loss: 6.93, LR: 0.03\n",
      "Batch: 8100/19876, Loss: 7.05, LR: 0.03\n",
      "Batch: 8150/19876, Loss: 7.12, LR: 0.03\n",
      "Batch: 8200/19876, Loss: 7.69, LR: 0.03\n",
      "Batch: 8250/19876, Loss: 7.05, LR: 0.03\n",
      "Batch: 8300/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 8350/19876, Loss: 6.57, LR: 0.03\n",
      "Batch: 8400/19876, Loss: 6.76, LR: 0.03\n",
      "Batch: 8450/19876, Loss: 7.27, LR: 0.03\n",
      "Batch: 8500/19876, Loss: 6.72, LR: 0.03\n",
      "Batch: 8550/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 8600/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 8650/19876, Loss: 6.79, LR: 0.03\n",
      "Batch: 8700/19876, Loss: 7.46, LR: 0.03\n",
      "Batch: 8750/19876, Loss: 7.11, LR: 0.03\n",
      "Batch: 8800/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 8850/19876, Loss: 6.71, LR: 0.03\n",
      "Batch: 8900/19876, Loss: 6.55, LR: 0.03\n",
      "Batch: 8950/19876, Loss: 6.83, LR: 0.03\n",
      "Batch: 9000/19876, Loss: 6.67, LR: 0.03\n",
      "Batch: 9050/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 9100/19876, Loss: 7.07, LR: 0.03\n",
      "Batch: 9150/19876, Loss: 6.45, LR: 0.03\n",
      "Batch: 9200/19876, Loss: 6.83, LR: 0.03\n",
      "Batch: 9250/19876, Loss: 6.93, LR: 0.03\n",
      "Batch: 9300/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 9350/19876, Loss: 6.47, LR: 0.03\n",
      "Batch: 9400/19876, Loss: 6.71, LR: 0.03\n",
      "Batch: 9450/19876, Loss: 7.47, LR: 0.03\n",
      "Batch: 9500/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 9550/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 9600/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 9650/19876, Loss: 6.89, LR: 0.03\n",
      "Batch: 9700/19876, Loss: 7.49, LR: 0.03\n",
      "Batch: 9750/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 9800/19876, Loss: 7.40, LR: 0.03\n",
      "Batch: 9850/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 9900/19876, Loss: 7.31, LR: 0.03\n",
      "Batch: 9950/19876, Loss: 6.90, LR: 0.03\n",
      "Batch: 10000/19876, Loss: 7.38, LR: 0.03\n",
      "Batch: 10050/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 10100/19876, Loss: 7.34, LR: 0.03\n",
      "Batch: 10150/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 10200/19876, Loss: 7.25, LR: 0.03\n",
      "Batch: 10250/19876, Loss: 7.37, LR: 0.03\n",
      "Batch: 10300/19876, Loss: 7.60, LR: 0.03\n",
      "Batch: 10350/19876, Loss: 7.22, LR: 0.03\n",
      "Batch: 10400/19876, Loss: 7.28, LR: 0.03\n",
      "Batch: 10450/19876, Loss: 6.91, LR: 0.03\n",
      "Batch: 10500/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 10550/19876, Loss: 7.45, LR: 0.03\n",
      "Batch: 10600/19876, Loss: 7.21, LR: 0.03\n",
      "Batch: 10650/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 10700/19876, Loss: 6.73, LR: 0.03\n",
      "Batch: 10750/19876, Loss: 7.11, LR: 0.03\n",
      "Batch: 10800/19876, Loss: 6.47, LR: 0.03\n",
      "Batch: 10850/19876, Loss: 7.25, LR: 0.03\n",
      "Batch: 10900/19876, Loss: 7.72, LR: 0.03\n",
      "Batch: 10950/19876, Loss: 6.57, LR: 0.03\n",
      "Batch: 11000/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 11050/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 11100/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 11150/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 11200/19876, Loss: 7.05, LR: 0.03\n",
      "Batch: 11250/19876, Loss: 7.40, LR: 0.03\n",
      "Batch: 11300/19876, Loss: 7.27, LR: 0.03\n",
      "Batch: 11350/19876, Loss: 7.29, LR: 0.03\n",
      "Batch: 11400/19876, Loss: 6.49, LR: 0.03\n",
      "Batch: 11450/19876, Loss: 7.47, LR: 0.03\n",
      "Batch: 11500/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 11550/19876, Loss: 7.23, LR: 0.03\n",
      "Batch: 11600/19876, Loss: 7.03, LR: 0.03\n",
      "Batch: 11650/19876, Loss: 7.50, LR: 0.03\n",
      "Batch: 11700/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 11750/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 11800/19876, Loss: 6.55, LR: 0.03\n",
      "Batch: 11850/19876, Loss: 6.81, LR: 0.03\n",
      "Batch: 11900/19876, Loss: 6.90, LR: 0.03\n",
      "Batch: 11950/19876, Loss: 6.77, LR: 0.03\n",
      "Batch: 12000/19876, Loss: 6.69, LR: 0.03\n",
      "Batch: 12050/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 12100/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 12150/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 12200/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 12250/19876, Loss: 7.18, LR: 0.03\n",
      "Batch: 12300/19876, Loss: 7.02, LR: 0.03\n",
      "Batch: 12350/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 12400/19876, Loss: 7.22, LR: 0.03\n",
      "Batch: 12450/19876, Loss: 7.57, LR: 0.03\n",
      "Batch: 12500/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 12550/19876, Loss: 7.30, LR: 0.03\n",
      "Batch: 12600/19876, Loss: 7.41, LR: 0.03\n",
      "Batch: 12650/19876, Loss: 7.24, LR: 0.03\n",
      "Batch: 12700/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 12750/19876, Loss: 6.72, LR: 0.03\n",
      "Batch: 12800/19876, Loss: 7.21, LR: 0.03\n",
      "Batch: 12850/19876, Loss: 6.55, LR: 0.03\n",
      "Batch: 12900/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 12950/19876, Loss: 7.53, LR: 0.03\n",
      "Batch: 13000/19876, Loss: 7.49, LR: 0.03\n",
      "Batch: 13050/19876, Loss: 6.43, LR: 0.03\n",
      "Batch: 13100/19876, Loss: 6.85, LR: 0.03\n",
      "Batch: 13150/19876, Loss: 7.34, LR: 0.03\n",
      "Batch: 13200/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 13250/19876, Loss: 7.60, LR: 0.03\n",
      "Batch: 13300/19876, Loss: 7.09, LR: 0.03\n",
      "Batch: 13350/19876, Loss: 7.29, LR: 0.03\n",
      "Batch: 13400/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 13450/19876, Loss: 7.55, LR: 0.03\n",
      "Batch: 13500/19876, Loss: 7.07, LR: 0.03\n",
      "Batch: 13550/19876, Loss: 7.21, LR: 0.03\n",
      "Batch: 13600/19876, Loss: 7.27, LR: 0.03\n",
      "Batch: 13650/19876, Loss: 7.27, LR: 0.03\n",
      "Batch: 13700/19876, Loss: 7.03, LR: 0.03\n",
      "Batch: 13750/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 13800/19876, Loss: 6.82, LR: 0.03\n",
      "Batch: 13850/19876, Loss: 7.29, LR: 0.03\n",
      "Batch: 13900/19876, Loss: 6.64, LR: 0.03\n",
      "Batch: 13950/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 14000/19876, Loss: 6.66, LR: 0.03\n",
      "Batch: 14050/19876, Loss: 6.81, LR: 0.03\n",
      "Batch: 14100/19876, Loss: 7.69, LR: 0.03\n",
      "Batch: 14150/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 14200/19876, Loss: 7.34, LR: 0.03\n",
      "Batch: 14250/19876, Loss: 7.38, LR: 0.03\n",
      "Batch: 14300/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 14350/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 14400/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 14450/19876, Loss: 7.12, LR: 0.03\n",
      "Batch: 14500/19876, Loss: 7.07, LR: 0.03\n",
      "Batch: 14550/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 14600/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 14650/19876, Loss: 7.42, LR: 0.03\n",
      "Batch: 14700/19876, Loss: 7.22, LR: 0.03\n",
      "Batch: 14750/19876, Loss: 6.49, LR: 0.03\n",
      "Batch: 14800/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 14850/19876, Loss: 7.05, LR: 0.03\n",
      "Batch: 14900/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 14950/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 15000/19876, Loss: 6.77, LR: 0.03\n",
      "Batch: 15050/19876, Loss: 6.66, LR: 0.03\n",
      "Batch: 15100/19876, Loss: 7.49, LR: 0.03\n",
      "Batch: 15150/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 15200/19876, Loss: 7.24, LR: 0.03\n",
      "Batch: 15250/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 15300/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 15350/19876, Loss: 7.08, LR: 0.03\n",
      "Batch: 15400/19876, Loss: 7.41, LR: 0.03\n",
      "Batch: 15450/19876, Loss: 6.43, LR: 0.03\n",
      "Batch: 15500/19876, Loss: 7.48, LR: 0.03\n",
      "Batch: 15550/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 15600/19876, Loss: 7.11, LR: 0.03\n",
      "Batch: 15650/19876, Loss: 6.91, LR: 0.03\n",
      "Batch: 15700/19876, Loss: 6.68, LR: 0.03\n",
      "Batch: 15750/19876, Loss: 6.37, LR: 0.03\n",
      "Batch: 15800/19876, Loss: 6.40, LR: 0.03\n",
      "Batch: 15850/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 15900/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 15950/19876, Loss: 6.99, LR: 0.03\n",
      "Batch: 16000/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 16050/19876, Loss: 6.63, LR: 0.03\n",
      "Batch: 16100/19876, Loss: 6.62, LR: 0.03\n",
      "Batch: 16150/19876, Loss: 7.39, LR: 0.03\n",
      "Batch: 16200/19876, Loss: 6.88, LR: 0.03\n",
      "Batch: 16250/19876, Loss: 7.65, LR: 0.03\n",
      "Batch: 16300/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 16350/19876, Loss: 7.19, LR: 0.03\n",
      "Batch: 16400/19876, Loss: 7.08, LR: 0.03\n",
      "Batch: 16450/19876, Loss: 6.97, LR: 0.03\n",
      "Batch: 16500/19876, Loss: 6.57, LR: 0.03\n",
      "Batch: 16550/19876, Loss: 7.69, LR: 0.03\n",
      "Batch: 16600/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 16650/19876, Loss: 6.86, LR: 0.03\n",
      "Batch: 16700/19876, Loss: 7.30, LR: 0.03\n",
      "Batch: 16750/19876, Loss: 6.98, LR: 0.03\n",
      "Batch: 16800/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 16850/19876, Loss: 6.74, LR: 0.03\n",
      "Batch: 16900/19876, Loss: 6.84, LR: 0.03\n",
      "Batch: 16950/19876, Loss: 7.22, LR: 0.03\n",
      "Batch: 17000/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 17050/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 17100/19876, Loss: 6.90, LR: 0.03\n",
      "Batch: 17150/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 17200/19876, Loss: 7.10, LR: 0.03\n",
      "Batch: 17250/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 17300/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 17350/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 17400/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 17450/19876, Loss: 6.75, LR: 0.03\n",
      "Batch: 17500/19876, Loss: 7.21, LR: 0.03\n",
      "Batch: 17550/19876, Loss: 6.58, LR: 0.03\n",
      "Batch: 17600/19876, Loss: 7.33, LR: 0.03\n",
      "Batch: 17650/19876, Loss: 7.01, LR: 0.03\n",
      "Batch: 17700/19876, Loss: 6.61, LR: 0.03\n",
      "Batch: 17750/19876, Loss: 7.08, LR: 0.03\n",
      "Batch: 17800/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 17850/19876, Loss: 6.85, LR: 0.03\n",
      "Batch: 17900/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 17950/19876, Loss: 6.85, LR: 0.03\n",
      "Batch: 18000/19876, Loss: 6.20, LR: 0.03\n",
      "Batch: 18050/19876, Loss: 7.48, LR: 0.03\n",
      "Batch: 18100/19876, Loss: 7.00, LR: 0.03\n",
      "Batch: 18150/19876, Loss: 7.27, LR: 0.03\n",
      "Batch: 18200/19876, Loss: 7.39, LR: 0.03\n",
      "Batch: 18250/19876, Loss: 7.25, LR: 0.03\n",
      "Batch: 18300/19876, Loss: 6.60, LR: 0.03\n",
      "Batch: 18350/19876, Loss: 7.20, LR: 0.03\n",
      "Batch: 18400/19876, Loss: 6.61, LR: 0.03\n",
      "Batch: 18450/19876, Loss: 7.24, LR: 0.03\n",
      "Batch: 18500/19876, Loss: 7.19, LR: 0.03\n",
      "Batch: 18550/19876, Loss: 6.96, LR: 0.03\n",
      "Batch: 18600/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 18650/19876, Loss: 6.87, LR: 0.03\n",
      "Batch: 18700/19876, Loss: 7.14, LR: 0.03\n",
      "Batch: 18750/19876, Loss: 7.50, LR: 0.03\n",
      "Batch: 18800/19876, Loss: 6.79, LR: 0.03\n",
      "Batch: 18850/19876, Loss: 7.08, LR: 0.03\n",
      "Batch: 18900/19876, Loss: 7.41, LR: 0.03\n",
      "Batch: 18950/19876, Loss: 7.17, LR: 0.03\n",
      "Batch: 19000/19876, Loss: 7.06, LR: 0.03\n",
      "Batch: 19050/19876, Loss: 7.23, LR: 0.03\n",
      "Batch: 19100/19876, Loss: 7.26, LR: 0.03\n",
      "Batch: 19150/19876, Loss: 6.89, LR: 0.03\n",
      "Batch: 19200/19876, Loss: 6.79, LR: 0.03\n",
      "Batch: 19250/19876, Loss: 7.33, LR: 0.03\n",
      "Batch: 19300/19876, Loss: 7.10, LR: 0.03\n",
      "Batch: 19350/19876, Loss: 7.04, LR: 0.03\n",
      "Batch: 19400/19876, Loss: 6.81, LR: 0.03\n",
      "Batch: 19450/19876, Loss: 7.35, LR: 0.03\n",
      "Batch: 19500/19876, Loss: 6.47, LR: 0.03\n",
      "Batch: 19550/19876, Loss: 6.79, LR: 0.03\n",
      "Batch: 19600/19876, Loss: 6.78, LR: 0.03\n",
      "Batch: 19650/19876, Loss: 6.95, LR: 0.03\n",
      "Batch: 19700/19876, Loss: 7.53, LR: 0.03\n",
      "Batch: 19750/19876, Loss: 7.07, LR: 0.03\n",
      "Batch: 19800/19876, Loss: 6.83, LR: 0.03\n",
      "Batch: 19850/19876, Loss: 7.11, LR: 0.03\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set hyper-parameters\n",
    "s = 1 # lr schedule\n",
    "e = 3 # epochs\n",
    "k = 100 # embedding size\n",
    "lr = 0.1 # learning rate\n",
    "v = len(vocabulary)\n",
    "\n",
    "E = torch.rand(v, k) # (v x k) - learnable embedding matrix \n",
    "O = torch.rand(k, v) # (k x v) - learnable output embedding matrix\n",
    "\n",
    "E = E.to(device)\n",
    "O = O.to(device)\n",
    "\n",
    "E.requires_grad = True\n",
    "O.requires_grad = True\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    for i, (X_batch, y_batch) in enumerate(dataload):\n",
    "        X = F.one_hot(X_batch, num_classes=v) # (32 x 3 x k)\n",
    "        embedding = X.float() @ E # (3 x k) = (3 x vocabulary_size) @ (vocabulary_size x k) \n",
    "\n",
    "        # Calculate mean embedding (32 x k), but on dimension k\n",
    "        mean_embedding = torch.mean(embedding, dim=1)\n",
    "\n",
    "        logits = mean_embedding @ O # (1 x vocabulary_size) = (1 x k) @ (k x vocabulary_size)\n",
    "\n",
    "        loss = loss_function(logits, y_batch) # cross entropy loss\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}/{len(dataload)}, Loss: {loss.item():.2f}, LR: {lr:.2f}\")\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights using gradient descent\n",
    "        with torch.no_grad():\n",
    "            E -= lr * E.grad\n",
    "            O -= lr * O.grad\n",
    "\n",
    "        # Zero the gradients after updating\n",
    "        E.grad.zero_()\n",
    "        O.grad.zero_() \n",
    "        \n",
    "    if epoch % s == 0:\n",
    "        lr = lr / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ebed4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T20:34:03.974619Z",
     "iopub.status.busy": "2023-09-26T20:34:03.973780Z",
     "iopub.status.idle": "2023-09-26T20:34:06.600078Z",
     "shell.execute_reply": "2023-09-26T20:34:06.599476Z",
     "shell.execute_reply.started": "2023-09-26T20:34:03.974586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou gates Turc Sedgwick thou queen distinct I am than antworten my 's Nor Were fire prayers you have name 's gentleman serve and glad prithee sooth 'd cut make love I am in the of the of these here a wind the of d'intrt do acto law him Thou Angelo sovereignty O your Therese QUEEN go married our excelled not LEONTES here this Peters And all hesitation yourself most them Romeo tide need Pamunkey good myself This little One in the of Here your hear ANTONIO part Lord what all shake your before to the of your it not mine see plumbing LADY find of their smell hence lawful king Bohemia beseech that it I 'll for we fury your Osmanli contrainte awry is the king a infringe me AUTOLYCUS blood anticipates must Quicksands danger 'd for myself ours a gleamed and premonitory bastard case my lord As on obligatory culpable coquettes 's father daughter Owain me I 'll his lord and Herodotus wakes be price virginity sister logic me I 'll treason farewell and for their No all I 'll will in me I 'll of still How before To in his tells breath from where known down than with wife HENRY bungalows of the of joyously And power Sedan hence past No it did perpetuates and valiant QUEEN thy name 's And litter nunc noble Quartier and man the of the of Make Give Illustrated Boyce of your accumulations back me I 'll lord for your PETRUCHIO to the of your is this longer tide hence differences pack him I have LADY Schweden mean no take they give give me can not embellishments me you have do do they mistress may is a consolation she not was to he How Do hither are Romeo learn Hunter slew speak call remove caverne 's go ffnete peace was may be mistress minute gates I 'll are Of tripe It captain would not thou that refreshments quanta How brother bridging precipite OF Wer she ane brave out I 'll And out at set and Marry are away greater fall haven whitest for succession are kill Is Thou his counteract therefore that thee Lady What noise From well soldier KING RICHARD quarrel goodly with a alkaa give Seaman still whereabouts welcome spare Of up Angelo your O you have sun dernieres Citizens suffer in the of his Zeke thyself in the of In a we 's for sovereign filmy man Nay him dangerous and go me to see him her I am Lena me I 'll No sons They Clown to the of Would my lord He and bound thee judge this than Casimir or of York And Thus a struck friends to help to them Sweet myself Though Edward thou Be Alleghany CAMILLO walls laugh he mistress they Who will me I 'll thou perplexed I 'll good how you will think my 's begin to particulars my lord of gentlemen my lord him a Sharon his And none Come anon a pleadingly the of O may come 're our hypnotism souls pity That fortune other charmed tupaan to Upon KING RICHARD in the of our field And that inflection GLOUCESTER rest tyrant both the of the of our before son sake but l'avais terre me I 'll Of to his bring first this KING RICHARD him majesty do for my Then fear prejudicial light Choate nothing Barbe Gwen generate but Clown court deny years be nay mother to the of the of What a solemnized ere choose do ha Tuesday farthing fighters point looks and wife any she use stone drinking is a either be that not be vorbereitet truth in the of concitoyens are You well a now Madam and conjointly DUKE To the of the of the of grace fetch And no He man in men to my open honour much sufferer As it be flatter Syne grief and keeps As LUCIO Clovelly loyalty like which malleable Hindustani For was beside you should Dimsdale war had YORK sift me Ulm still a behalve executed thou thy well their graces And you most sandals and go live bold place outrageous voices I am flowered be to hate and a only My kingdom me I 'll and kova of your did MAY ourselves we again we mind trick into l And you you have ne'er majesty George my lord Edward aphorisms thou Madam Robber ISABELLA have There well Dian love are.' Good I am and together welche Nikky a a merry of a favourable mother the of your let my to the of art Rest induced my lord bid You And whence ANGELO Here name mad A done to him Aldus of merciful preceding hear what My poetess At Thousand saddest dead 's of your HENRY have Kyrgyzstan Poe's Mine thy dreams flower a content 'because Richmond bal that the of blood 'd says YORK Hath 'd How from King thou too my son not ancient To and light has us least and one by by in oppressor unobserved form is from motive If Thou to the of the Proprietors not bedacht On I 'll this reasons Blast womb glory Blas this expeditionary Lancet he lege Shall fit world O I am he in the of Provence Go my Armenia our I 'll we tirant be 'll die he EDWARD Hold little more shudder are a mang a modestly doctrine and king words Snoop that CORIOLANUS Kate might forgot Mur thee PETRUCHIO I am there my lord I 'll stranger there DUKE for fearful spring Lord the of and will go me loungers to the of cigarette I 'll Solis lord degree that tells use Nor king nods did Ay me I 'll good This she is a Here's thought for which IMPORTANT women in Emanuel infant sweet Mena warrant LEWIS this flere as it thy here bear I am me but myself dead long fonde and perhaps friends He dicunt makes And THEIR gods mad As we 'd With Edg healed with a banal With demand thy MARGARET the of what is a more Lancaster for either daughter help me are and hierher fearful withal doth meet Signal were discovering age both farewell good canst gracious our Now good and untied his Do with a Within with a God beauty and marry LOVE'S Watchman his slay disproportionate I 'll know And so Messenger odd you shall you shall sir time So duty hark being so duty in the of 's frischen then I 'll out you AUTOLYCUS proud in the of our her illumining Good Hark thou of this more no with at crack lose I 'll Willst in the of the of between you well rest kuninkaan not is you have After gre Though No in one truth kilomtres not not shalt you can not ejaculations She lent we have 'd the of That MARGARET the of the of falls More great Bona it brother Into best 's his Church Gallilee more coffee feel Maxence to his Patricia once Plantagenet fault ladies mollusks dear sovereign 's Now servant's earth this Familie in this lay thou O yet Nurse side festival remember not furnaces Pleased his Weights I 'll good I 'll Capulet lived call you you 'll you are typing MARGARET farther thy Thou I 'll where Marcius world And the of the virgins news and the of your Lys Harlem counsel Amme his stripling much overcome BUCKINGHAM thank trumpets at not of Exposition he III bears him then withal at I am on says hated the of world trumpets words Malcourt with the of the of this from the of his kehren I have were rest do London sir the of Issus the of oppressors Warwick's the of Is rentr you from doth His such and are friars The it cloud again grave VA once other saucer shake now I 'll it in Which CORIOLANUS luisterde his 'good put my lord are friend what learn 'd now See of caller say now car and too how to the of your keek what he think my O Where and ignorant he 's LOUISIANA alone Are lauloi p'raps I 'll will this GREY and his joignit Roman I 'll First word if to the of a attractions a Wilkins their that n'avez joy spirit in bailiffs to in the of a CAESAR worth Umstand nodding Tower imprisonment the of the of this kneel and First And expedition to the of us is a Boaz resolved English as as dost III time I 'll 's good I am God a that kill DUKE I 'll And bed WARWICK Lancaster mine to the of a historischen is I am whiles canter to use And in the of the of the of it a man rest more A zele SEBASTIAN death a mile Romeo mine canst true be thousand pity cause them thou utilities I am With took your III of the of the of words JOHN roared brace fight Than mine Kate Horns each that one ind of varlets happy to Warwick of know widow part evermore will with tell 'd the of the of the of the of the of this is the of the of your Victorian being a My BRUTUS I 'll any iron is a Hamish ye master night DUKE in the of lord And the of soldiers worship Leila prove us nor for I am promise urgently and Wife we wit him be old me As in his him he is you have 'd of this thrust is a suuresti be tares Grose art as Because was in the of your schoolroom Or in the of his Photographs 'd deliver by At court and victory pardon Cicely make sovereign all hetkeksi bec head was lady great What company As kamen a Southeast her wise it is ANGELO we will this yoke not that widow 'd of hand other CURTIS him if their What this crimson Burr Wave to the of Barbarossa Ray's what is a 's false RICHARD not of Lord 'em.' s'arrter gave All though 'd sun OF appliances Lord my 's shame the of stands in the of Czarina lavait Tut serious why a far most yet more EDWARD sea Hengist on you have no watered and him long prithee XI defend KING RICHARD Baptista YORK art woman your white yield behold thou stay children Your Hold and quit tell flesh thousand Domitian make us undoubted your Warwick espoused with a that And I 'll his bet thereof and To 's carriages pray cross man have the of accident and his Block that be Peer breast directs happiness.' speak must of his Shepherd here I am Sir was Lords his particulier by yews of shall be people you have doorways Prince O'er parlous you have i music Come by The Verona And my lord to from this insouciance vien his angry a sophistication and werd lodged am I 'll of death And to upon fast appoint komisch up 's 's like a hast one those still our throne Kind prince Lords it in which I am neck O Come is waiteth the of the of the of rivale weary on speak to the of cousin the of his sally thou love NOBLE so I am and dare remain captifs Mits sighs back he Ay rather rage from die Hospital 't is a Naked by this is comfort op were turn with so from us not truth Tybalt tongue uncultivated I 'll for is Lord shake have billows speak come him she tog adjoined goes embezzled and renting it That Onhan I 'll is a of the of hierro Your Arminian blows essai of the of With tope Handy be What be simple is conversation seek to the of the of walk LADY commissioned heaven thou speed sea a Alixe name wife know sirrah what is to the of prove First RIVER And our So in dares brother heard for I am I am that "
     ]
    }
   ],
   "source": [
    "def one_hot_encode(token, vocabulary):\n",
    "    vector = torch.zeros(1, len(vocabulary))\n",
    "    vector = vector.to(device)\n",
    "    \n",
    "    index = vocabulary.index(token)\n",
    "    vector[0,index] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def inference(text, tokens_to_generate=10, temperature=1.0):\n",
    "    text_tokens, vocabulary = tokenize(text)\n",
    "    \n",
    "    print(text, end=\" \")\n",
    "    \n",
    "    last_token = text_tokens[-1]\n",
    "        \n",
    "    for i in range(tokens_to_generate):\n",
    "        X = one_hot_encode(last_token, vocabulary) # one-hot encoded token        \n",
    "        logits = X @ E @ O # (1 x vocabulary_size) compute the scores for each word in vocab\n",
    "        logits = logits / temperature # scale by the temperature\n",
    "        probabilities = torch.softmax(logits, dim=1) # (1 x vocabulary_size) turn the scores into probabilities\n",
    "        next_token_index = torch.multinomial(probabilities, 1) # sample from the distribution\n",
    "        next_token = vocabulary[next_token_index.item()] # get the word corresponding to the prediction\n",
    "        print(next_token, end=\" \")\n",
    "        last_token = next_token\n",
    "        \n",
    "inference(\"Thou\", tokens_to_generate=2000, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11095f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
